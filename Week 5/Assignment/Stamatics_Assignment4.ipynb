{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "## Regression Recap\n",
        "\n",
        "Solve this question on Kaggle! Submit your predictions in Kaggle.\n",
        "Download the notebook and submit it separately along with the other questions.\n",
        "\n",
        "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview\n",
        "\n",
        "Use everything that you have learnt so far.\n",
        "\n",
        "Analyse the data. Drop, scale or normalize features. Merge features if they correlated to make some meaningful features. Use PCA to reduce the dimensionality of the problem.\n",
        "\n",
        "Try out all the regression models. Use cross validation to find best parameters for each model."
      ],
      "metadata": {
        "id": "V8HfbPwRtl9c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1dPS7qnhtla1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6toho7uRO7v"
      },
      "source": [
        "# Question 2\n",
        "## Developing an Artificial Neural Network from Scratch.\n",
        "\n",
        "In this notebook, we will be developing a fully connected feedforward neural network.\n",
        "\n",
        "We will import the MNIST dataset from keras datsets. The MNIST dataset contains images of 28x28 pixels each having values ranging from 0-255.\n",
        "It has 60000 images in the training set and 10000 images in the test set. However, we will only use the first 10000 images for training and first 1000 images for testing because our code isn't optimized and it takes time to run. We are not looking for accuracy of our network right now, we will be doing that in the next question when we will be implementing the same using Tensorflow.\n",
        "\n",
        "\n",
        "Run the first 3 cells. Your code begins after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI17X78rktdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrINntzulT4M",
        "outputId": "72879853-cec4-4d1e-dd27-245e969cd0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the class, the images are flattened to a column.\n",
        "\n",
        "Then we are normalizing them by dividing by 255."
      ],
      "metadata": {
        "id": "dr4rLzb9ZBQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jy7CLWCEwfn"
      },
      "outputs": [],
      "source": [
        "train_X=train_X.reshape(60000,784,1)    # flattening\n",
        "test_X=test_X.reshape(10000,784,1)\n",
        "\n",
        "train_y=train_y.reshape(60000,1)\n",
        "test_y=test_y.reshape(10000,1)\n",
        "\n",
        "train_X= train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X=train_X[:10000]         #taking the first 10000 images.\n",
        "train_y=train_y[:10000]\n",
        "test_X=test_X[:1000]\n",
        "test_y=test_y[:1000]\n",
        "train_data=list(zip(train_X,train_y))\n",
        "test_data=list(zip(test_X,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Write the code for Sigmoid Function."
      ],
      "metadata": {
        "id": "wWwDzh6kZOy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q5a8tGYku-7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return \"\"\"....\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 The Network\n",
        "\n",
        "We will making a class called Network which has certain functions inside it. The cost function used is Cross-Entropy Loss. You need to code only the first 3. Rest are done for you.  There are various places within the code marked as stop_zone. Read the instructions below the code at those places to check whether your code till there is correct or not."
      ],
      "metadata": {
        "id": "cIJI5SoxbJaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwsydmyTEt0z"
      },
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self,sizes): # sizes is a list containing the network.\n",
        "                              # eg : [784,128,10] means input =784 neurons,\n",
        "                              #    1st hidden layer 128 neurons, output 10 neurons.\n",
        "        self.sizes=sizes\n",
        "        self.num_layers=\"...can you say the number of layers based on the list called sizes...\"\n",
        "        self.weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "        self.biases= \"...can you do this by understanding the self.weights...\"\n",
        "\n",
        "# stop_zone 1. Comment out all the code below. Select all rows below. Click Ctrl + /.\n",
        "# Include the show function given below above this comment area inside the class.\n",
        "# Run this cell and then run the code with stop_zone 1 written below.\n",
        "# After this testing, don't forget to remove the comments. Same, select all, Ctrl+/.\n",
        "\n",
        "    def forwardpropagation(self,a):\n",
        "        for b,w in zip(self.biases, self.weights):\n",
        "            a=\"...a is activation...\" # sig (w.a +b)\n",
        "            # print(a.shape)\n",
        "        return a\n",
        "\n",
        "# stop_zone 2. Comment out all the code below. Don't comment out the __init__ method else you will get error.\n",
        "# Remove comment from print(a.shape) line above. Run this cell. And run the code with stop_zone 2 written below.\n",
        "\n",
        "\n",
        "    def backpropagation(self,x,y):\n",
        "\n",
        "        # nothing to do in this 3 lines. it is for creating a one-hot encoded vector of the labels.\n",
        "        y_t = np.zeros((len(y), 10))\n",
        "        y_t[np.arange(len(y)), y] = 1\n",
        "        y_t= y_t.T\n",
        "\n",
        "        #nabla_b=dC/db and nabla_w=dC/dw. They are lists of shapes equal to that of bias and weights.\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # initially, a0 = input.\n",
        "        activation=x\n",
        "        activation_list=[x]\n",
        "\n",
        "        # step 1 : calculation of delta in last layer\n",
        "\n",
        "        # write the same forward propagation code here but while doing so store the a's.\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            activation= \"...just written above...\"\n",
        "            activation_list.append(activation)\n",
        "\n",
        "        delta= \"...delta is dC/dz3... how is it calculated?...\"\n",
        "\n",
        "        # step 2 : nabla_b and nabla_w relation with delta of last layer\n",
        "\n",
        "        nabla_b[-1]=\"...how is dC/db3 and dC/dz3 related...\"\n",
        "        nabla_w[-1]= \"...how is dC/dw3 and dC/dz3 related...\"\n",
        "\n",
        "        # print(\"{} {}\".format(nabla_b[-1].shape,nabla_w[-1].shape) )\n",
        "#stop_zone 3 : remove comment from the print statement just above and run the cell for stop_zone3.\n",
        "# don't forget commenting out.\n",
        "\n",
        "        # step 3 : calculation of delta for hidden layers\n",
        "\n",
        "        for j in range(2,self.num_layers):\n",
        "            sig_der = activation_list[-j]*(1-activation_list[-j])\n",
        "            delta= \"...how is dC/dz2 and dC/dz3 related ? Look i have calculated one term already for you (sig_der)...\"\n",
        "\n",
        "            # step 4 : nabla_b and nabla_w relation with delta of others layers\n",
        "            nabla_b[-j]= \"...again, how is dC/db2 and dC/dz2 related...\"\n",
        "            nabla_w[-j]= \"...how is dC/dw2 and dC/dz2 related...\"\n",
        "\n",
        "        return (nabla_b,nabla_w)\n",
        "#stop_zone 4 : Run the cell for stop_zone 4.\n",
        "\n",
        "    def SGD(self, train_data,epochs,mini_batch_size, lr):\n",
        "        n_train= len(train_data)\n",
        "        for i in range(epochs):\n",
        "            random.shuffle(train_data)\n",
        "            mini_batches = [\"\"\"...Split the data into batches each of size = mini_batch_size  ...\"\"\"]\n",
        "\n",
        "  # Stop zone 5 : Remove comment from the next print line and comment out all the lines below it.\n",
        "        # print(np.array(mini_batches, dtype=object).shape)\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch,lr)\n",
        "\n",
        "            self.predict(train_data)\n",
        "            print(\"Epoch {0} completed.\".format(i+1))\n",
        "\n",
        "    # the functions below are complete. If you are fine till stop_zone 5, you can run\n",
        "    # this whole cell and train, test the data by running the last cell of the notebook.\n",
        "    # You may need to wait for around 10 minutes to see the test predictions.\n",
        "\n",
        "    def update_mini_batch(self,mini_batch,lr):\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "        for x,y in mini_batch:\n",
        "            delta_b,delta_w= self.backpropagation(x,y)\n",
        "            nabla_b=[nb+ db for nb,db in zip (nabla_b,delta_b)]\n",
        "            nabla_w=[nw+dw for nw,dw in zip(nabla_w,delta_w)]\n",
        "\n",
        "        self.weights=[w- lr*nw/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
        "        self.biases=[b-lr*nb/len(mini_batch) for b,nb in zip(self.biases,nabla_b)]\n",
        "\n",
        "    def predict(self,test_data):\n",
        "        test_results = [(np.argmax(self.forwardpropagation(x)),y) for x,y in test_data]\n",
        "        # returns the index of that output neuron which has highest activation\n",
        "\n",
        "        num= sum(int (x==y) for x,y in test_results)\n",
        "        print (\"{0}/{1} classified correctly.\".format(num,len(test_data)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u8cVnGamVgP"
      },
      "outputs": [],
      "source": [
        "# stop_zone 1\n",
        "\n",
        "def show(self):\n",
        "  print(self.num_layers)\n",
        "  for bias in self.biases:\n",
        "      print(bias.shape)\n",
        "  for weight in self.weights:\n",
        "      print(weight.shape)\n",
        "\n",
        "# Copy this show function from here. Paste it inside that Network Class.\n",
        "# Comment out the show function here. Run this cell.\n",
        "\n",
        "net=Network([784,128,64,10])\n",
        "net.show()\n",
        "\n",
        "# The desired output is :\n",
        "# 4\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation.\n",
        "\n",
        "# Keeping the show function over there in the Network class doesn't make any\n",
        "# difference. You may delete it if you wish. Better toss a coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7EJBF7XsSft"
      },
      "outputs": [],
      "source": [
        "# stop_zone 2\n",
        "# to use this, make sure your data is loaded. Run this cell.\n",
        "net=Network([784,128,64,10])\n",
        "print(train_X[0])\n",
        "net.forwardpropagation(train_X[0])\n",
        "\n",
        "# The desired output is :\n",
        "# (784, 1)\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_zone 3\n",
        "net=Network([784,128,64,10])\n",
        "net.backpropagation(train_X[0],train_y[0])\n",
        "\n",
        "# Desired output : (10,1) (10,64)"
      ],
      "metadata": {
        "id": "FwHWyaKNhIIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_zone 4\n",
        "net=Network([784,128,64,10])\n",
        "nabla_b,nabla_w=net.backpropagation(train_X[0],train_y[0])\n",
        "for nb in nabla_b:\n",
        "  print(nb.shape)\n",
        "for nw in nabla_w:\n",
        "  print(nw.shape)\n",
        "\n",
        "# Desired output:\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)"
      ],
      "metadata": {
        "id": "9pq4E3rHik-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop zone 5 :  Run this cell, for 10000 samples and batch size of 20, output should be\n",
        "#       (500,20,2).  500 batches each of size 20 and has 2 objects : train and test data.\n",
        "\n",
        "net=Network([784,256,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=20,mini_batch_size=20,lr=0.01)"
      ],
      "metadata": {
        "id": "FaCblO7XE0Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXljiAYRlvdq"
      },
      "outputs": [],
      "source": [
        "net=Network([784,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=10,mini_batch_size=20,lr=0.01)\n",
        "print(\"Test data:\")\n",
        "net.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of question 2."
      ],
      "metadata": {
        "id": "mhMIoFT9m7OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3\n",
        "\n",
        "1. Load the MNIST dataset as in the previous question.\n",
        "2. Visualize the first 20 images of both train and test data using plt.imshow()\n",
        "3. Choose any of tensorflow and pytorch.\n",
        "4. The architecture :\n",
        "> input layer (optional) <br/>\n",
        "> flatten layer <br/>\n",
        "> some hidden dense layers (tensorflow) <br/> **OR** <br/>\n",
        "> some linear layers with non linear activation (pytorch) <br/>\n",
        "> output dense/linear layer of 10 neurons.\n",
        "\n",
        "5. For Tensorflow or PyTorch, you may either use Sequential API or Functional API to define the model.<br/>\n",
        "However, for PyTorch, you need to load the data as a torch Dataset and access using a DataLoader function and then iterate over the data to train.\n",
        "\n",
        "6. Normalize the images before training.\n",
        "7. While training, use Adam Optimizer and Sparse Categorical Cross Entropy Loss function (Tensorflow) / Cross Entropy Loss (Pytorch)."
      ],
      "metadata": {
        "id": "b2aXYawgIIZP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0GjG9LnYT88S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}