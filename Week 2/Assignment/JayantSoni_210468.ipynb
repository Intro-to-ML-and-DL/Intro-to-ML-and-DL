{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "\n",
        "Make a class called LinearRegression which provides two functions : fit and predict. You may use the code present in the repository as template. You need to make the following changes in order to include regularization :\n",
        "1. To initialize an object of the class, you need to provide 4 parameters : learning_rate, epochs, penalty and alpha (coefficient of the regularization term). Penalty and alpha should have default values of None and 0 respectively.\n",
        "2. The parameter penalty should take in any one of these inputs : L1 (Lasso), L2 (Ridge) and None (simple LR).\n",
        "3. Do some basic differentiation to find out the expressions of dC/dw and dC/db when regularization is involved. Use internet whenever necessary.\n",
        "4. Write if-else statements inside the fit method to cover the different values for dw for different values of penalty."
      ],
      "metadata": {
        "id": "8G9yCczBb1uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate,epochs,penalty='None',alpha=0):\n",
        "        self.lr=learning_rate\n",
        "        self.epochs=epochs\n",
        "        self.penalty = penalty\n",
        "        self.alpha=alpha\n",
        "        \n",
        "    def fit(self, X_train, y_train):\n",
        "        n_samples, n_features = X_train.shape\n",
        "        y_train=y_train.reshape(-1,1)\n",
        "        # init parameters\n",
        "        self.weights = np.zeros((n_features,1))\n",
        "        self.bias = np.zeros((1,1))\n",
        "        \n",
        "        # gradient descent\n",
        "        for i in range(self.epochs):\n",
        "            delta= -2*(y_train-np.dot(X_train,self.weights)-self.bias)/n_samples\n",
        "            dw=0\n",
        "            if self.penalty == 'l1':\n",
        "                dw= np.dot(X_train.T,delta)+ self.alpha\n",
        "            elif self.penalty == 'l2':\n",
        "                dw= np.dot(X_train.T,delta) + 2*self.alpha*np.sum(self.weights)\n",
        "            else:\n",
        "                dw= np.dot(X_train.T,delta)\n",
        "            \n",
        "            db= np.sum(delta).reshape(1,1)\n",
        "\n",
        "            #update weights and biases\n",
        "            self.weights-= self.lr * dw\n",
        "            self.bias-= self.lr* db\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_predicted = np.dot(X_test,self.weights)+self.bias\n",
        "        return y_predicted"
      ],
      "metadata": {
        "id": "4_AmGXZbb24d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n",
        "Use the dataset https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction (*).\n",
        "1. Read it using pandas.\n",
        "2. Check for **null values**.\n",
        "3. For each of the columns (except the first and last), plot the column values in the X-axis against the last column of prices in the Y-axis.\n",
        "4. Remove the unwanted columns.\n",
        "5. Split the dataset into train and test data. Test data size = 25% of total dataset.\n",
        "6. **Normalize** the X_train and X_test using MinMaxScaler from sklearn.preprocessing.\n",
        "7. Fit the training data into the 3 models created in question 1 (**linear regression, lasso and ridge regression**) and predict the testing data.\n",
        "8. Use **mean square error and R<sup>2</sup>** from sklearn.metrics as evaluation criterias.\n",
        "9. Fit the training data into the models of the same name provided by sklearn.linear_model and evaluate the predictions using MSE and R<sup>2</sup>. You may require to convert the data to numpy arrays if any error arises.\n",
        "10. Tune the hyperparameters of your models (learning rate, epochs, penalty and alpha) to achieve losses close to that of the sklearn models. (*We will cover hyperparameter tuning using GridSearchCV and all in later weeks. For now, you may manually run the model for different values of the hyperparameters.*)\n",
        "\n",
        "Note : (*) To solve this question, you may proceed in any of the following ways :\n",
        "1. Prepare the notebook in Kaggle, download it and submit it separately with the other questions.\n",
        "2. Download the dataset from kaggle. Upload it to the session storage in Colab.\n",
        "3. Use Colab data directly in Colab. [Refer here](https://www.kaggle.com/general/74235). For this, you need to create kaggle API token. Before submitting, hide or remove the API token."
      ],
      "metadata": {
        "id": "j-XkGETQblG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-24T20:05:15.561670Z",
          "iopub.execute_input": "2023-05-24T20:05:15.562096Z",
          "iopub.status.idle": "2023-05-24T20:05:15.586075Z",
          "shell.execute_reply.started": "2023-05-24T20:05:15.562060Z",
          "shell.execute_reply": "2023-05-24T20:05:15.584624Z"
        },
        "trusted": true,
        "id": "1JuBa9lfbeEL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/real-estate-price-prediction/Real estate.csv\")\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:05:15.590317Z",
          "iopub.execute_input": "2023-05-24T20:05:15.591298Z",
          "iopub.status.idle": "2023-05-24T20:05:15.612146Z",
          "shell.execute_reply.started": "2023-05-24T20:05:15.591263Z",
          "shell.execute_reply": "2023-05-24T20:05:15.611276Z"
        },
        "trusted": true,
        "id": "uMU5KlzzbeEV",
        "outputId": "8156ef81-b8ca-46c4-f70b-ae26213884e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-71b2fb3e13d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/real-estate-price-prediction/Real estate.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/real-estate-price-prediction/Real estate.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:05:15.650274Z",
          "iopub.execute_input": "2023-05-24T20:05:15.651130Z",
          "iopub.status.idle": "2023-05-24T20:05:15.671361Z",
          "shell.execute_reply.started": "2023-05-24T20:05:15.651097Z",
          "shell.execute_reply": "2023-05-24T20:05:15.670255Z"
        },
        "trusted": true,
        "id": "3FEmgbzsbeEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = list(df.columns)\n",
        "cols.remove('No')\n",
        "cols.remove('Y house price of unit area')\n",
        "cols.remove('X1 transaction date')\n",
        "cols"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:05:15.679364Z",
          "iopub.execute_input": "2023-05-24T20:05:15.679753Z",
          "iopub.status.idle": "2023-05-24T20:05:15.687896Z",
          "shell.execute_reply.started": "2023-05-24T20:05:15.679721Z",
          "shell.execute_reply": "2023-05-24T20:05:15.686803Z"
        },
        "trusted": true,
        "id": "J-THAhvYbeEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "i=1\n",
        "for col in cols:\n",
        "    plt.subplot(2,3,i)\n",
        "    i+=1\n",
        "    plt.scatter(df[col],df['Y house price of unit area'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:05:15.692570Z",
          "iopub.execute_input": "2023-05-24T20:05:15.693567Z",
          "iopub.status.idle": "2023-05-24T20:05:16.498205Z",
          "shell.execute_reply.started": "2023-05-24T20:05:15.693482Z",
          "shell.execute_reply": "2023-05-24T20:05:16.497022Z"
        },
        "trusted": true,
        "id": "nyDwPXQLbeEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df[cols]\n",
        "y=df['Y house price of unit area']\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.25,random_state=10)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:14:33.326561Z",
          "iopub.execute_input": "2023-05-24T20:14:33.327069Z",
          "iopub.status.idle": "2023-05-24T20:14:33.340297Z",
          "shell.execute_reply.started": "2023-05-24T20:14:33.327025Z",
          "shell.execute_reply": "2023-05-24T20:14:33.338900Z"
        },
        "trusted": true,
        "id": "Xa8dsoG3beEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=np.array(X_train)\n",
        "X_test=np.array(X_test)\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:15:05.678494Z",
          "iopub.execute_input": "2023-05-24T20:15:05.678974Z",
          "iopub.status.idle": "2023-05-24T20:15:05.685812Z",
          "shell.execute_reply.started": "2023-05-24T20:15:05.678936Z",
          "shell.execute_reply": "2023-05-24T20:15:05.684617Z"
        },
        "trusted": true,
        "id": "ZvBT4Ci7beEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler= MinMaxScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:15:08.528485Z",
          "iopub.execute_input": "2023-05-24T20:15:08.529288Z",
          "iopub.status.idle": "2023-05-24T20:15:08.537844Z",
          "shell.execute_reply.started": "2023-05-24T20:15:08.529238Z",
          "shell.execute_reply": "2023-05-24T20:15:08.536447Z"
        },
        "trusted": true,
        "id": "RlRsJJdubeEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg1= LinearRegression(learning_rate=0.01,epochs=3000)\n",
        "reg2= LinearRegression(learning_rate=0.01,epochs=3000,penalty='l1',alpha=0.015)\n",
        "reg3= LinearRegression(learning_rate=0.01,epochs=3000,penalty='l2',alpha=0.015)\n",
        "\n",
        "reg1.fit(X_train,y_train)\n",
        "reg2.fit(X_train,y_train)\n",
        "reg3.fit(X_train,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:17:17.320400Z",
          "iopub.execute_input": "2023-05-24T20:17:17.320895Z",
          "iopub.status.idle": "2023-05-24T20:17:17.791096Z",
          "shell.execute_reply.started": "2023-05-24T20:17:17.320858Z",
          "shell.execute_reply": "2023-05-24T20:17:17.789634Z"
        },
        "trusted": true,
        "id": "Clqr0wZwbeEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred=reg1.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))\n",
        "\n",
        "y_pred=reg2.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))\n",
        "\n",
        "y_pred=reg3.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:17:17.845278Z",
          "iopub.execute_input": "2023-05-24T20:17:17.845700Z",
          "iopub.status.idle": "2023-05-24T20:17:17.857930Z",
          "shell.execute_reply.started": "2023-05-24T20:17:17.845666Z",
          "shell.execute_reply": "2023-05-24T20:17:17.856739Z"
        },
        "trusted": true,
        "id": "PfmQJSsnbeEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression as Linreg,Lasso,Ridge\n",
        "\n",
        "reg1= Linreg()\n",
        "reg2= Lasso()\n",
        "reg3= Ridge()\n",
        "\n",
        "reg1.fit(X_train,y_train)\n",
        "reg2.fit(X_train,y_train)\n",
        "reg3.fit(X_train,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:15:47.075361Z",
          "iopub.execute_input": "2023-05-24T20:15:47.075798Z",
          "iopub.status.idle": "2023-05-24T20:15:47.228493Z",
          "shell.execute_reply.started": "2023-05-24T20:15:47.075769Z",
          "shell.execute_reply": "2023-05-24T20:15:47.227168Z"
        },
        "trusted": true,
        "id": "wasmHKzEbeEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=reg1.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))\n",
        "\n",
        "y_pred=reg2.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))\n",
        "\n",
        "y_pred=reg3.predict(X_test)\n",
        "print(mse(y_test,y_pred))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-24T20:15:48.800542Z",
          "iopub.execute_input": "2023-05-24T20:15:48.800979Z",
          "iopub.status.idle": "2023-05-24T20:15:48.813735Z",
          "shell.execute_reply.started": "2023-05-24T20:15:48.800948Z",
          "shell.execute_reply": "2023-05-24T20:15:48.812257Z"
        },
        "trusted": true,
        "id": "D3DxAJzlbeEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3\n",
        "\n",
        "The breast cancer dataset is a binary classification dataset commonly used in machine learning tasks. It is available in scikit-learn (sklearn) as part of its datasets module.\n",
        "Here is an explanation of the breast cancer dataset's components:\n",
        "\n",
        "* Features (X):\n",
        "\n",
        " * The breast cancer dataset consists of 30 numeric features representing different characteristics of the FNA images. These features include mean, standard error, and worst (largest) values of various attributes such as radius, texture, smoothness, compactness, concavity, symmetry, fractal dimension, etc.\n",
        "\n",
        "* Target (y):\n",
        "\n",
        " * The breast cancer dataset is a binary classification problem, and the target variable (y) represents the diagnosis of the breast mass. It contains two classes:\n",
        "    * 0: Represents a malignant (cancerous) tumor.\n",
        "    * 1: Represents a benign (non-cancerous) tumor.\n",
        "\n",
        "Complete the code given below in place of the \"...\""
      ],
      "metadata": {
        "id": "wmEvWOi1cKFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the dataset from sklearn.datasets\n",
        "2. Separate out the X and Y columns.\n",
        "3. Create a train-test-split. Take any suitable test size."
      ],
      "metadata": {
        "id": "f2rq-IqkjMRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "Y=data.target\n",
        "X = data.data\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "S0x12c7wmKVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X=scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "iEZbdFiqs8cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "bNObLTnnm2hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write code for the sigmoid function and Logistic regression.\n",
        "(Logistic Regression code is available in the Week2/Examples folder. However, try to code it yourself. A template is provided for that.)\n",
        "\n",
        "*Optional* : Include the regularization terms as you did in the first question. "
      ],
      "metadata": {
        "id": "LYLC5gxfjQGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(z):\n",
        "    a=1.0/(1.0+ np.exp(-z))\n",
        "    return a\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate,epochs,penalty='None',alpha=0):\n",
        "        self.lr=learning_rate\n",
        "        self.epochs=epochs\n",
        "        self.penalty = penalty\n",
        "        self.alpha=alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.reshape(-1, 1)\n",
        "        self.weights=np.random.randn(n_features,1)/np.sqrt(n_features)\n",
        "        self.bias= np.random.randn(1,1)\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            z = np.dot(X,self.weights) + self.bias\n",
        "            y_pred = sigmoid(z)\n",
        "\n",
        "            \n",
        "            dw = -np.dot(X.T,(y - y_pred))/n_samples\n",
        "\n",
        "            if self.penalty == 'l1':\n",
        "                dw += self.alpha\n",
        "            elif self.penalty == 'l2':\n",
        "                dw += 2*self.alpha*np.sum(self.weights)\n",
        "                \n",
        "            db = -np.sum(y - y_pred)/n_samples\n",
        "            self.weights -= self.lr* dw\n",
        "            self.bias-= self.lr* db\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.dot(X,self.weights)+self.bias\n",
        "        Y_proba_pred=y_pred\n",
        "        for i in range(len(y_pred)):\n",
        "            if y_pred[i]<= 0.5:\n",
        "                y_pred[i] = 0\n",
        "            else:\n",
        "                y_pred[i] = 1   \n",
        "        return (Y_proba_pred,y_pred)"
      ],
      "metadata": {
        "id": "08ZkvQSbjRKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Fit your model on the dataset and make predictions.\n",
        "6. Compare your model with the Sklearn Logistic Regression model. Try out all the different penalties.\n",
        "7. Print accuracy_score in each case using sklearn.metrics ."
      ],
      "metadata": {
        "id": "eqhW1EWPjV2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of LogisticRegression\n",
        "logreg1 = LogisticRegression(0.1,3000) #Play around with different learning rates and epochs\n",
        "logreg2 = LogisticRegression(0.1,3000,penalty='l1',alpha=0.01)\n",
        "logreg3 = LogisticRegression(0.1,3000,penalty='l1',alpha=0.01)\n",
        "\n",
        "# Train the model\n",
        "logreg1.fit(X_train,y_train)\n",
        "logreg2.fit(X_train,y_train)\n",
        "logreg3.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_proba_pred1,y_pred1 = logreg1.predict(X_test)\n",
        "Y_proba_pred2,y_pred2 = logreg2.predict(X_test)\n",
        "Y_proba_pred3,y_pred3 = logreg3.predict(X_test)"
      ],
      "metadata": {
        "id": "iaHkdoEMjZPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred1))\n",
        "print(accuracy_score(y_test,y_pred2))\n",
        "print(accuracy_score(y_test,y_pred3))"
      ],
      "metadata": {
        "id": "A5NVwmX7jeYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "logreg4 = LR(penalty=None)\n",
        "logreg5 = LR(penalty='l1',solver='saga')\n",
        "logreg6 = LR(penalty=\"l2\")\n",
        "logreg7 = LR(penalty=\"elasticnet\",l1_ratio=0.4,solver='saga')\n",
        "\n",
        "logreg4.fit(X_train,y_train)\n",
        "logreg5.fit(X_train,y_train)\n",
        "logreg6.fit(X_train,y_train)\n",
        "logreg7.fit(X_train,y_train)\n",
        "\n",
        "y_pred4 = logreg4.predict(X_test)\n",
        "y_pred5 = logreg5.predict(X_test)\n",
        "y_pred6 = logreg6.predict(X_test)\n",
        "y_pred7 = logreg6.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test,y_pred4))\n",
        "print(accuracy_score(y_test,y_pred5))\n",
        "print(accuracy_score(y_test,y_pred6))\n",
        "print(accuracy_score(y_test,y_pred7))"
      ],
      "metadata": {
        "id": "B35Caw4Qot7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. For the best model in each case (yours and scikit-learn), print the classification_report using sklearn.metrics .\n"
      ],
      "metadata": {
        "id": "Li-MoA7Rjbr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = data.target_names\n",
        "print(classification_report(y_test, y_pred1, target_names=target_names))\n",
        "print(classification_report(y_test, y_pred5, target_names=target_names))"
      ],
      "metadata": {
        "id": "gYqqczTYufcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. For the best model in each case (yours and scikit-learn), print the confusion_matrix using sklearn.metrics .\n"
      ],
      "metadata": {
        "id": "lxnI6gv8vQ0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred1))\n",
        "print(confusion_matrix(y_test, y_pred5))"
      ],
      "metadata": {
        "id": "KqBUUKfwvPn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Optional Challenge : For the best model in each case (yours and scikit-learn), print the roc_auc_score and plot the roc curves using sklearn.metrics and matplotlib."
      ],
      "metadata": {
        "id": "btMrbjJOvobb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "print(roc_auc_score(y_test, logreg5.predict_proba(X_test)[:, 1]))\n",
        "print(roc_auc_score(y_test, Y_proba_pred2))"
      ],
      "metadata": {
        "id": "uzI-sghavn4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,6))\n",
        "fpr1, tpr1, _ = roc_curve(y_test, logreg5.predict_proba(X_test)[:, 1], pos_label=1)\n",
        "fpr2, tpr2, _= roc_curve(y_test, Y_proba_pred2, pos_label=1)\n",
        "plt.plot(fpr1,tpr1,color='red')\n",
        "plt.plot(fpr2,tpr2,color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zD_-S390xEE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4\n",
        "\n",
        "How accurately can a K-Nearest Neighbors (KNN) model classify different types of glass based on a glass classification dataset consisting of 214 samples and 7 classes? Use the kaggle dataset \"https://www.kaggle.com/datasets/uciml/glass\". \n",
        "\n",
        "Context: This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)\n",
        "\n",
        "\n",
        "Build a knn model from scratch. Assign different colours to different classes and plot the data points using matplotlib. Also find the accuracy of the model.  \n",
        "\n"
      ],
      "metadata": {
        "id": "9cbNUcGzROmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap as lcm"
      ],
      "metadata": {
        "id": "xfs8axBQRYq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidian_distance(x,x1):\n",
        "  return np.sqrt(np.sum(np.subtract(x,x1)**2))"
      ],
      "metadata": {
        "id": "LUqje7fBR7gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self,k):\n",
        "        self.k=k\n",
        "    def fit(self,X_train,y_train):\n",
        "        self.X_train=X_train\n",
        "        self.y_train=y_train\n",
        "    def predict(self,X_test):\n",
        "        y_pred=[]\n",
        "        for x in X_test:\n",
        "            distance = [euclidian_distance(x,x1) for x1 in self.X_train]\n",
        "            k_shortest_indices = np.argsort(distance)[:self.k]\n",
        "            k_labels = [self.y_train[i] for i in k_shortest_indices]\n",
        "            common_label = np.bincount(k_labels)\n",
        "            y_pred.append(np.argmax(common_label))\n",
        "        return np.array(y_pred)\n",
        "    \n",
        "def accuracy(predictions,y_test):\n",
        "    return np.sum(predictions==y_test)/len(y_test)"
      ],
      "metadata": {
        "id": "RnCJvoznSFAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data= pd.read_csv('/content/glass.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "KUfUuj6WT2B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= data.drop('Type',axis=1)\n",
        "y= data['Type']\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)"
      ],
      "metadata": {
        "id": "GB8xnhM51cE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "bgmMTcUWZNdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=KNN(k=3)\n",
        "clf.fit(x_train,y_train)\n",
        "predictions=clf.predict(x_test)\n",
        "print(accuracy(predictions,y_test))"
      ],
      "metadata": {
        "id": "Vi_HI2gHYAPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "clf = knn(n_neighbors=3)\n",
        "clf.fit(x_train,y_train)\n",
        "pred=clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,pred)"
      ],
      "metadata": {
        "id": "rKRTCDcPZRiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier as dtc,plot_tree\n",
        "clf=dtc(max_depth=10,criterion=\"entropy\")\n",
        "clf.fit(x_train,y_train)\n",
        "pred=clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,pred)"
      ],
      "metadata": {
        "id": "x3A_GVJw1RmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tree(clf,max_depth=5)"
      ],
      "metadata": {
        "id": "-PcYOH-820OB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}