{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M7IlzQhajs71"
      },
      "source": [
        "###Question 1:\n",
        "Generate a dataset for linear regression with 1000 samples, 5 features and single target.\n",
        "\n",
        "Visualize the data by plotting the target column against each feature column. Also plot the best fit line in each case.\n",
        "\n",
        "Hint : search for obtaining regression line using numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-07o0-eHZU"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# generating data using make_regression of sklearn\n",
        "X, y = make_regression(n_samples = 1000, n_features = 5, n_targets = 1, noise = 0)\n",
        "# storing plots\n",
        "fig, ax = plt.subplots(5,1, figsize = (10,10))\n",
        "\n",
        "# running a for loop to generate all 5 scatter plots\n",
        "for i in range(5):\n",
        "    ax[i].scatter(X[:, i], y, color = 'b')\n",
        "    ax[i].set_xlabel(f'Feature {i+1}')\n",
        "    ax[i].set_ylabel('Target')\n",
        "    \n",
        "    # for best fit line in each case\n",
        "    a, b = np.polyfit(X[:, i], y, deg = 1)\n",
        "    \n",
        "    ax[i].axline(xy1 = (0, b), slope = a, color = 'r', label = f'y = {a:.2f}x + {b:.2f}')\n",
        "    ax[i].legend()\n",
        "\n",
        "# show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGDTvDVd57W"
      },
      "source": [
        "### Question 2:\n",
        "Make a classification dataset of 1000 samples with 2 features, 2 classes and 2 clusters per class.\n",
        "Plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DspQLHVeeH01"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# generating classification dataset using make_classification from sklearn\n",
        "X,y= make_classification(n_samples = 1000, n_features = 2, n_classes = 2, n_clusters_per_class = 2,  n_informative = 2, n_redundant = 0)\n",
        "\n",
        "# plotting data on scatter plot\n",
        "plt.scatter(X[:, 0],X[:,1], c=y)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7ghM2NebJXtR"
      },
      "source": [
        "### Question 3:\n",
        "Make a clustering dataset with 2 features and 4 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjjsnbxieIZN"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# generation cluster dataset using make_blobs of sklearn\n",
        "X,y = make_blobs(n_samples = 1000, centers = 4, n_features = 2)\n",
        "\n",
        "# plotting dataset on a scatter plot\n",
        "plt.scatter(X[:, 0], X[:,1], c = y)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eskxgE9T1jh2"
      },
      "source": [
        "## Question 4\n",
        "Go to the website https://www.worldometers.info/coronavirus/ and scrape the table containing covid-19 infection and deaths data using requests and BeautifulSoup.  Convert the table to a Pandas dataframe with the following columns : Country, Continent, Population, TotalCases, NewCases, TotalDeaths, NewDeaths,TotalRecovered, NewRecovered,  ActiveCases.\n",
        "\n",
        "*(<b>Optional Challenge :</b> Change the data type of the Columns (Population ... till ActiveCases) to integer. For that you need to remove the commas and plus signs. You may need to use df.apply() and pd.to_numeric() . Take care of the values which are empty strings.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7fs4Th9eI6W"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import requests as rq\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# setting up url, getting content from that url and store it in page variable\n",
        "url = \"https://www.worldometers.info/coronavirus\"\n",
        "page = rq.get(url)\n",
        "\n",
        "# parsing data to soup\n",
        "soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "# storing table from soup\n",
        "table = soup.find(\"table\", id = \"main_table_countries_today\")\n",
        "\n",
        "# collecting rows from table\n",
        "rows = table.find_all(\"tr\")\n",
        "\n",
        "# storing header of table\n",
        "header = [th.text for th in rows[0].find_all(\"th\")]\n",
        "\n",
        "# storing all the data of table in data variable using for loop\n",
        "data = []\n",
        "for row in rows[1:]:\n",
        "    data.append([td.text for td in row.find_all(\"td\")])\n",
        "\n",
        "# store this data in pandas DataFrame\n",
        "df = pd.DataFrame(data, columns = header)\n",
        "\n",
        "df = df[[\"Country,Other\", \"Continent\", \"Population\", \"TotalCases\", \"NewCases\", \"TotalDeaths\", \"NewDeaths\", \"TotalRecovered\", \"NewRecovered\", \"ActiveCases\"]]\n",
        "\n",
        "df.columns = [\"Country\", \"Continent\", \"Population\", \"TotalCases\", \"NewCases\", \"TotalDeaths\", \"NewDeaths\",\"TotalRecovered\", \"NewRecovered\", \"ActiveCases\"]\n",
        "\n",
        "# printing data\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QhHpN4yCxn-H"
      },
      "source": [
        "# Question 5\n",
        "\n",
        "Generate an imbalanced classification dataset using sklearn of 1000 samples with 2 features, 2 classes and 1 cluster per class. Plot the data. One of the class should contain only 5% of the total samples. Confirm this either using numpy or Counter. Plot the data.\n",
        "\n",
        "Now oversample the minority class to 5 times its initial size using SMOTE. Verify the number. Plot the data.\n",
        "\n",
        "Now undersample the majority class to 3 times the size of minority class using RandomUnderSampler. Verify the number. Plot the data.\n",
        "\n",
        "Reference : Last markdown cell of the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLKcLL42lCa2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X, y = make_blobs(n_samples=1000, n_features=2, centers=2, cluster_std=1.0, random_state=42)\n",
        "y[0:950] = 0\n",
        "y[950:] = 1\n",
        "\n",
        "print(\"Class distribution using numpy:\")\n",
        "print(np.bincount(y))\n",
        "\n",
        "print(\"Class distribution using Counter:\")\n",
        "print(Counter(y))\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap='rainbow')\n",
        "plt.title(\"Imbalanced classification dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "sm = SMOTE(sampling_strategy=0.25, random_state=42)\n",
        "X_sm, y_sm = sm.fit_resample(X, y)\n",
        "\n",
        "print(\"Number of samples after oversampling:\")\n",
        "print(X_sm.shape[0])\n",
        "\n",
        "print(\"Class distribution after oversampling using numpy:\")\n",
        "print(np.bincount(y_sm))\n",
        "\n",
        "print(\"Class distribution after oversampling using Counter:\")\n",
        "print(Counter(y_sm))\n",
        "\n",
        "plt.scatter(X_sm[:,0], X_sm[:,1], c=y_sm, cmap='rainbow')\n",
        "plt.title(\"Oversampled classification dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "rus = RandomUnderSampler(sampling_strategy=1.5, random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(X_sm, y_sm)\n",
        "\n",
        "print(\"Number of samples after undersampling:\")\n",
        "print(X_rus.shape[0])\n",
        "\n",
        "print(\"Class distribution after undersampling using numpy:\")\n",
        "print(np.bincount(y_rus))\n",
        "\n",
        "print(\"Class distribution after undersampling using Counter:\")\n",
        "print(Counter(y_rus))\n",
        "\n",
        "plt.scatter(X_rus[:,0], X_rus[:,1], c=y_rus, cmap='rainbow')\n",
        "plt.title(\"Undersampled classification dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j0Smzgk6mZ"
      },
      "source": [
        "##Question 6\n",
        "\n",
        "Write a Python code to perform data preprocessing on a dataset using the scikit-learn library. Follow the instructions below:\n",
        "\n",
        " * Load the dataset using the scikit-learn `load_iris` function.\n",
        " * Assign the feature data to a variable named `X` and the target data to a variable named `y`.\n",
        " * Create a pandas DataFrame called `df` using `X` as the data and the feature names obtained from the dataset.\n",
        " * Display the first 5 rows of the DataFrame `df`.\n",
        " *  Check if there are any missing values in the DataFrame and handle them accordingly.\n",
        " * Split the data into training and testing sets using the `train_test_split` function from scikit-learn. Assign 70% of the data to the training set and the remaining 30% to the testing set.\n",
        " * Print the dimensions of the training set and testing set respectively.\n",
        " *  Standardize the feature data in the training set using the `StandardScaler` from scikit-learn.\n",
        " *  Apply the same scaling transformation on the testing set.\n",
        " * Print the first 5 rows of the standardized training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCJg725i4xiY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Assign feature data and target data\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values:\", df.isnull().sum())\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print the dimensions of the training and testing sets\n",
        "print(\"Training Set Dimensions:\", X_train.shape)\n",
        "print(\"Testing Set Dimensions:\", X_test.shape)\n",
        "\n",
        "# Standardize the feature data in the training set\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the same scaling transformation on the testing set\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print the first 5 rows of the standardized training set\n",
        "df_train_scaled = pd.DataFrame(X_train_scaled, columns=iris.feature_names)\n",
        "print(df_train_scaled.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
