{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3ecf1-5da7-46f0-b36f-e1b96bd9f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 1:\n",
    "Generate a dataset for linear regression with 1000 samples, 5 features and single target.\n",
    "\n",
    "Visualize the data by plotting the target column against each feature column. Also plot the best fit line in each case.\n",
    "\n",
    "Hint : search for obtaining regression line using numpy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3496f143-bf29-43c8-b8db-c62511c1ab12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m#setting random seed for reproduciblity\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  #setting random seed for reproduciblity\n",
    "\n",
    "# generating random data \n",
    "NumSamples = 1000\n",
    "NumFeatures = 5\n",
    "\n",
    "#generating features X from a random normal distribution \n",
    "X= np.random.randn(NumSamples ,NumFeatures)\n",
    "\n",
    "#generating target values of Y from a linear combination of features with some noise\n",
    "true_coefficients = np.array([2,-1,0.5,1.5,-0.5])\n",
    "noise = np.random.randn(NumSamples)*0.5\n",
    "Y=np.dot(X, true_coefficients)+noise\n",
    "\n",
    "\n",
    "fig, axs = plt.sublots(NumFeatures, 1, figsize=(6,20))                     #visualising data\n",
    "\n",
    "for i in range(NumFeatures):\n",
    "    ax = axs[i]\n",
    "    ax.scatter(X[:,i],Y, s=5,alpha =0.5)\n",
    "    ax.set_xlabel('Feature {}'.format(i+1))\n",
    "    ax.set_ylabel('Target')\n",
    "    ax.set_title('Feature {} vs Target'.format(i+1))\n",
    "\n",
    "                                                  \n",
    "    coefficients= np.polyfit(X[:,i],Y,1)                                    #calculate best fit line\n",
    "    best_fit_line = np.polyval(coefficients, X[:,i])\n",
    "\n",
    "    ax.plot(X[:,i],best_fit_line, color ='blue')                            #plot best fit line\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66f24a-7819-43cc-96bd-a2316a68d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 2:\n",
    "Make a classification dataset of 1000 samples with 2 features, 2 classes and 2 clusters per class. Plot the data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb00ef-fe38-4b0a-a434-7c962655e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dataset import make_blobs\n",
    "\n",
    "np.random.seed(42)  #setting random seed for reproduciblity\n",
    " \n",
    "num_samples = 1000      #generating classification  dataset\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "num_cluster_per_class = 2\n",
    "\n",
    "x,y = make_blobs(n_samples =num_samples, n_features=num_features, centers = num_classes*num_cluster_per_class,cluster_std=1.0, center_box=(-10.0,10.0),random_state=42)\n",
    "\n",
    "#plot the Data\n",
    "plt.figure(figsize=(8,6))\n",
    "for class_label in range(num_classes):\n",
    "    class_samples = x[y == class_label]\n",
    "    plt.scatter(class_samples[:,0], class_samples[:,1], label='class {}'.format(class_label))\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('classification Dataset')\n",
    "plt.legend()\n",
    "plot.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a346c8-50d7-4939-ab73-ffcef7e48604",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 3:\n",
    "Make a clustering dataset with 2 features and 4 clusters.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b209b-ecac-442a-9a70-5d1522a75766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dataset import make_blobs\n",
    "\n",
    "np.random.seed(42)  #setting random seed for reproduciblity\n",
    " \n",
    "num_samples = 1000      #generating classification  dataset\n",
    "num_features = 2\n",
    "num_clusters = 4\n",
    "\n",
    "x,y = make_blobs(n_samples =num_samples, n_features=num_features, centers = num_clusters ,cluster_std=1.0, center_box=(-10.0,10.0),random_state=42)\n",
    "#plotting of data\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap='viridis')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('clustering Dataset')\n",
    "plt.grid(True)\n",
    "plot.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d8a89-a8ea-4ea7-a3d1-1b10c775efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 4\n",
    "Go to the website https://www.worldometers.info/coronavirus/ and scrape the table containing covid-19 infection and deaths data using requests and BeautifulSoup. Convert the table to a Pandas dataframe with the following columns : Country, Continent, Population, TotalCases, NewCases, TotalDeaths, NewDeaths,TotalRecovered, NewRecovered, ActiveCases.\n",
    "\n",
    "*(Optional Challenge : Change the data type of the Columns (Population ... till ActiveCases) to integer. For that you need to remove the commas and plus signs. You may need to use df.apply() and pd.to_numeric() . Take care of the values which are empty strings.)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a0c08-a2cd-4ee9-8938-59243633367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import panda as pd\n",
    "\n",
    "#send a get request to the website\n",
    "url = 'https://www.worldometers.info/coronavirus/'\n",
    "response = requests.get(url)\n",
    "\n",
    "#create a beatifulSoup objet to parse the html content \n",
    "soup = BeautifulSoup(respose.content, 'html.parser')\n",
    "\n",
    "#find the table element with class 'table' and extract the table data  \n",
    "table = soup.find('table',class_='table')\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "#Extract the table headers (column names)\n",
    "headers = []\n",
    "for row in table_rows[1:]:\n",
    "    data =[]\n",
    "    for td in row.find_all('td'):\n",
    "        data.append(td.text.strip())\n",
    "    rows.append(data)\n",
    "\n",
    "#creating a panda dataframe from extracted table data\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "#select desired columns\n",
    "columns_to_kepp =['Country,Other', 'Continent','Population','TotalCases','NewCases','TotalDeaths','NewDeaths','TotalRecovered', 'NewRecovered', 'ActiveCases']\n",
    "df= df[columns_to_keep]\n",
    "\n",
    "#Print the resulting dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e610c60-4b26-40b6-9f84-292af64ef531",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Question 6\n",
    "Write a Python code to perform data preprocessing on a dataset using the scikit-learn library. Follow the instructions below:\n",
    "\n",
    "Load the dataset using the scikit-learn load_iris function.\n",
    "Assign the feature data to a variable named X and the target data to a variable named y.\n",
    "Create a pandas DataFrame called df using X as the data and the feature names obtained from the dataset.\n",
    "Display the first 5 rows of the DataFrame df.\n",
    "Check if there are any missing values in the DataFrame and handle them accordingly.\n",
    "Split the data into training and testing sets using the train_test_split function from scikit-learn. Assign 70% of the data to the training set and the remaining 30% to the testing set.\n",
    "Print the dimensions of the training set and testing set respectively.\n",
    "Standardize the feature data in the training set using the StandardScaler from scikit-learn.\n",
    "Apply the same scaling transformation on the testing set.\n",
    "Print the first 5 rows of the standardized training set.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3da5be-07a7-4129-b960-cd829c4165f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScalar\n",
    "\n",
    "#Load the dataSet \n",
    "iris = load_iris()\n",
    "\n",
    "#assign the feature data to xand target data to y\n",
    "X= iris.data\n",
    "y=iris.target\n",
    "\n",
    "#Create a pandas DataFrame called df using X as the data and the feature names obtained from the dataset.\n",
    "df = pd.DataFrame(X,column=iris.feature_names)\n",
    "\n",
    "#Display the first 5 rows of the DataFrame df.\n",
    "print(df.head())\n",
    "\n",
    "#Check if there are any missing values in the DataFrame and handle them accordingly.\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#Split the data into training and testing sets using the train_test_split function from scikit-learn. Assign 70% of the data to the training set and the remaining 30% to the testing set.\n",
    "X_train , X_test, y_train , y_test = train_test_split(X,y,test_size= 0.3, random_state=42)\n",
    "\n",
    "#Print the dimensions of the training set and testing set respectively.\n",
    "print(\"Training set dimensions:\", X_train.shape)\n",
    "print(\"Testing set dimensions:\", X_test.shape)\n",
    "\n",
    "#Standardize the feature data in the training set using the StandardScaler from scikit-learn.\n",
    "scalar= StandardScalar()\n",
    "X_train_scaled =scaler.fit_transform(X_train)\n",
    "\n",
    "#Apply the same scaling transformation on the testing set.\n",
    "X_test_scaled =scalar.transform(X_test)\n",
    "\n",
    "#Print the first 5 rows of the standardized training set.\n",
    "print(pd.DataFrame(X_train_scaled, columns=iris.feature_names).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
