{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : Classification using Naive Bayes\n",
        "\n",
        "Can glucose and blood pressure data classify whether a patient has diabetes or not ? If yes, which classification algorithm should you use ?\n",
        "\n",
        "The dataset **diabetes_classification.csv** has 3 columns and 995 entries with the above data.\n",
        "\n",
        "\n",
        "1. Load the dataset."
      ],
      "metadata": {
        "id": "1aabbIB92YA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/diabetes_classification.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "cOqSpVYzmT7j",
        "outputId": "9e0f5863-d8d9-4600-8d44-abd955b45a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2ad8ae49a223>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/diabetes_classification.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/diabetes_classification.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The dataset has two feature columns and one target column. Plot a bar graph or histogram showing the distribution of values in the feature columns (count of each value)."
      ],
      "metadata": {
        "id": "vRcqHcd_5XUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(df['glucose'])\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(df['bloodpressure'])"
      ],
      "metadata": {
        "id": "XI_RImeHml-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The feature column **glucose** has a somewhat Gaussian distribution of data. So we will try out Gaussian Naive Bayes classification for the data using Scikit-Learn.\n",
        "\n",
        "3. Split the dataset.\n",
        "4. Fit a Gaussian NB model on the data. Make predictions and find the accuracy score.\n",
        "\n",
        "Optional :\n",
        "5. Compare the model with other classification algorithms like Logistic Regression, KNN, decision tree etc."
      ],
      "metadata": {
        "id": "Xa-7s9hL5tii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('diabetes',axis=1), df['diabetes'])"
      ],
      "metadata": {
        "id": "CDTogsUBoYAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train,y_train)\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,pred))"
      ],
      "metadata": {
        "id": "6NrELrg4pGX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prior(df, Y):\n",
        "    classes = sorted(list(df[Y].unique()))\n",
        "    prior = []\n",
        "    for i in classes:\n",
        "        prior.append(len(df[df[Y]==i])/len(df))\n",
        "    return prior\n",
        "\n",
        "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
        "    feat = list(df.columns)\n",
        "    df = df[df[Y]==label]\n",
        "    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n",
        "    return p_x_given_y\n",
        "\n",
        "def naive_bayes_categorical(df, X, Y):\n",
        "    # get feature names\n",
        "    features = list(df.columns)[:-1]\n",
        "\n",
        "    # calculate prior\n",
        "    prior = calculate_prior(df, Y)\n",
        "\n",
        "    Y_pred = []\n",
        "    # loop over every data sample\n",
        "    for x in X:\n",
        "        # calculate likelihood\n",
        "        labels = sorted(list(df[Y].unique()))\n",
        "        likelihood = [1]*len(labels)\n",
        "        for j in range(len(labels)):\n",
        "            for i in range(len(features)):\n",
        "                likelihood[j] *= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
        "\n",
        "        # calculate posterior probability (numerator only)\n",
        "        post_prob = [1]*len(labels)\n",
        "        for j in range(len(labels)):\n",
        "            post_prob[j] = likelihood[j] * prior[j]\n",
        "\n",
        "        Y_pred.append(np.argmax(post_prob))\n",
        "\n",
        "    return np.array(Y_pred)\n",
        "\n",
        "train, test = train_test_split(df, random_state=41)\n",
        "Y_pred = naive_bayes_categorical(train, test.iloc[:,:-1].values, \"diabetes\")\n",
        "print(accuracy_score(y_test, Y_pred))"
      ],
      "metadata": {
        "id": "PAbMgeymq8pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 : Regression using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **insurance.csv** file which contain information on insurance charges based on the following informations: age,sex,bmi,region,number of children and whether the person is a smoker or not. You need to predict the charges based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ],
      "metadata": {
        "id": "qSniomr219vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/insurance.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0YiiZ-cu7ext"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Separate the numerical and categorical columns.\n",
        "### 3. Label Encode the categorical columns.\n",
        "### 4. Scale the numerical columns. (Scale the charges separately so that you can calculate errors afterwards.)"
      ],
      "metadata": {
        "id": "t3kLtntR2_wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_columns = ['sex','smoker','region']\n",
        "num_columns = ['age','bmi']\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "for col in cat_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "num_data = df[num_columns]\n",
        "num_data = scaler.fit_transform(df[num_columns])\n",
        "df[num_columns]= num_data\n",
        "\n",
        "scaler2 = StandardScaler()\n",
        "charges = np.array(df['charges']).reshape(-1,1)\n",
        "charges = scaler2.fit_transform(charges)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "T8IgsLmS__RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Split the data."
      ],
      "metadata": {
        "id": "KAr0_8zS3d4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop('charges',axis=1), charges, test_size=0.2)"
      ],
      "metadata": {
        "id": "dlZ7gYBcskKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Support Vector Regressor\n",
        "\n",
        "Here , you will use the SVR model from sklearn.svm and fit it on the training data. Then predict on the test data and calaculate MAE, MSE. But...\n",
        "\n",
        "The SVR class contains many hyperparameters, example : kernel can have the following values : linear, rbf, poly, sigmoid.\n",
        "\n",
        "Use **RandomizedSearchCV** from sklearn.model_selection , create a dictionary with keys 'kernel' and 'gamma' . As values of the keys, create a list of some possible values. Run a 3-fold cross validation test (cv=3) and find the best parameters. Then initiate the SVR model with those parameters."
      ],
      "metadata": {
        "id": "9JKtZ3nN3gmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "params_grid = {\n",
        "    'kernel':['linear','rbf','poly'],\n",
        "    'gamma':np.logspace(-3, 3, 6)\n",
        "}\n",
        "\n",
        "svr = SVR()\n",
        "svr_cv =RandomizedSearchCV(svr, params_grid, cv=3)\n",
        "svr_cv.fit(X_train,y_train)\n",
        "print(svr_cv.best_score_, svr_cv.best_params_)\n"
      ],
      "metadata": {
        "id": "0vl9mqoJsqN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svr = SVR(kernel='poly', gamma=0.25)\n",
        "svr.fit(X_train,y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "y_pred= scaler2.inverse_transform(y_pred.reshape(-1,1))\n",
        "y_test =scaler2.inverse_transform(y_test)\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "print(mean_absolute_error(y_test,y_pred), mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "RBIFSpeBx_H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. AdaBoost Regressor\n",
        "\n",
        "We would do similar for AdaBoostRegressor from sklearn.ensemble . Here, the hyperparameters are n_estimators and loss.\n",
        "\n",
        "Instead of RandomizedSearchCV, let's try GridSearchCV . Find the best parameters and then find errors on test data using the model with best parameters."
      ],
      "metadata": {
        "id": "KGt147zA5E6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "adbr = AdaBoostRegressor()\n",
        "param_grid_adaboost = {\n",
        "     'n_estimators' : [20,40,50,60],\n",
        "     'loss': ['linear', 'square']\n",
        "}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "adbr_cv = GridSearchCV(adbr, param_grid_adaboost, cv=3)\n",
        "adbr_cv.fit(X_train,y_train)\n",
        "print(adbr_cv.best_estimator_ , adbr_cv.best_params_)"
      ],
      "metadata": {
        "id": "0UpLFdXcsulf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adbr = AdaBoostRegressor(loss= 'linear', n_estimators=50)\n",
        "adbr.fit(X_train,y_train)\n",
        "y_pred = adbr.predict(X_test)\n",
        "y_pred= scaler2.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "print(mean_absolute_error(y_test,y_pred), mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "ewnXZ9HQytt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Now carry the same procedure for Random Forest Regressor and for Gradient Boosting Regression.\n",
        "9. Finally, use <a href=\"https://xgboost.readthedocs.io/en/stable/get_started.html\"> XGBoost Regressor </a> and compare all the models. Comment which model had the least error (MAE and MSE).\n",
        "You will be required to run  <code> !pip install xgboost </code> to import xgboost models."
      ],
      "metadata": {
        "id": "oW11faHs5nH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred= scaler2.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "print(mean_absolute_error(y_test,y_pred), mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "V76R5aIzvkzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor()\n",
        "gbr.fit(X_train,y_train)\n",
        "\n",
        "y_pred = gbr.predict(X_test)\n",
        "y_pred= scaler2.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "print(mean_absolute_error(y_test,y_pred), mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "qAPZ0HoozA_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "XGxKEWhh3Ju5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://xgboost.readthedocs.io/en/stable/get_started.html"
      ],
      "metadata": {
        "id": "uPiPOsfy3j5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "bst = XGBRegressor()\n",
        "bst.fit(X_train, y_train)\n",
        "y_pred = bst.predict(X_test)\n",
        "\n",
        "y_pred= scaler2.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "print(mean_absolute_error(y_test,y_pred), mean_squared_error(y_test,y_pred))"
      ],
      "metadata": {
        "id": "9sJ-9N8t2vlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : Classification using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **bookmyshow_ads.csv** file which contain information on whether an url is spam or not based on 32 features. You need to classify the url as spam or not spam based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ],
      "metadata": {
        "id": "kMMzdz1e3YMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0Zu6iukU6oxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "-hCsNhXQCSAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split the data."
      ],
      "metadata": {
        "id": "BcqJmRVyLHFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= df.drop(columns=['index','Result'])\n",
        "Y= df['Result']"
      ],
      "metadata": {
        "id": "4Tx81MOHCzrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)"
      ],
      "metadata": {
        "id": "H47u7br9C0sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Comparison\n",
        "\n",
        "Similar to the previous question, use the following classifier models from sklearn and compare them:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Adaboost\n",
        "4. Gradient Boost\n",
        "5. XGBoost\n",
        "\n",
        "For each model, you may also try to find the best hyperparameters using GridSearch Cross Validation or RandomizedSearch Cross Validation."
      ],
      "metadata": {
        "id": "KKX0BSiILMPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "params_grid = {\n",
        "    'criterion':['gini', 'entropy'],\n",
        "    'max_depth':[20,30,50]\n",
        "}\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree_cv =RandomizedSearchCV(dtree, params_grid, cv=3, scoring='accuracy')\n",
        "dtree_cv.fit(X_train,y_train)\n",
        "print(dtree_cv.best_score_, dtree_cv.best_params_)\n"
      ],
      "metadata": {
        "id": "sjGH6EvsC_JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier(max_depth=20, criterion= 'entropy')\n",
        "dtree.fit(X_train,y_train)\n",
        "y_pred = dtree.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "aAJV7ZeOH969"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "param_grid_rf = {\n",
        "    'criterion':['gini','entropy'],\n",
        "    'max_features':['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rf_cv = GridSearchCV(rf, param_grid_rf, cv=3)\n",
        "rf_cv.fit(X_train,y_train)\n",
        "print(rf_cv.best_estimator_ , rf_cv.best_params_)"
      ],
      "metadata": {
        "id": "XU55pJFEDtIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(criterion='entropy', max_features='log2')\n",
        "rfc.fit(X_train,y_train)\n",
        "y_pred = rfc.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "JzMMjuSsJmFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "adbc = AdaBoostClassifier()\n",
        "adbc.fit(X_train,y_train)\n",
        "y_pred = adbc.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "tcuCYnw5DZ9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(X_train,y_train)\n",
        "\n",
        "y_pred = gbc.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "l6yhguBZJtyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "bst = XGBClassifier()\n",
        "bst.fit(X_train, y_train)\n",
        "y_pred = bst.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "nUnPhZkrKK6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 : Clustering\n",
        "\n",
        "Customer Segmentation is the subdivision of a market into discrete customer groups that share similar characteristics. Customer Segmentation can be a powerful means to identify unsatisfied customer needs.\n",
        "\n",
        "The csv file **segmentation data.csv** contains basic data about some customers like Customer ID, age, gender, annual income and spending score. You want to classify the customers into different groups so that marketing strategy could be planned in the future accordingly. How many different groups should be made ? What should be the approach ?\n",
        "\n",
        "This is an Unsupervised Learning question since it doesn't provide you with labels - the groups.\n",
        "\n",
        "### 1. Import the necessary modules"
      ],
      "metadata": {
        "id": "AbMIyf8ouk1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS4wxFeN9UuU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Read the csv file \"segmentation data.csv\" present in the Github repository as a Pandas DataFrame."
      ],
      "metadata": {
        "id": "FXs9rQC0u3_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"segmentation data.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "uy9kByrC9uFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Do the necessary preprocessing of the data.\n",
        "\n",
        "> Drop unwanted columns.\n",
        "\n",
        "> Check for null values.\n",
        "\n",
        "> Scale the numerical columns.\n",
        "\n",
        "> Additionally, you may also make the Age column have categorical values. How ? Apply some function that makes age groups turns all ages in some group to a particular number !\n",
        "\n",
        "Note : Don't do everything in a single code block ! Do it step-by-step and show output for each step."
      ],
      "metadata": {
        "id": "pkoXSkGvvEfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= data.drop('ID',axis=1)"
      ],
      "metadata": {
        "id": "PWvsi5349zUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "WWpkh9Wr96xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "income = np.array(data['Income']).reshape(-1,1)\n",
        "income = scaler.fit_transform(income)\n",
        "data['Income'] = income\n",
        "data.head()"
      ],
      "metadata": {
        "id": "SQ6CDFUF-O-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_age_category(x):\n",
        "    ans= (x - (x%10))/10\n",
        "    return int(ans)\n",
        "\n",
        "data['Age']= data['Age'].apply(get_age_category)"
      ],
      "metadata": {
        "id": "P23P9mXt-mny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "9W51gXzpD9jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. KMeans Model Training - Scikit-Learn\n",
        "\n",
        "At first, let's try to implement KMeans Clustering using sklearn.clusters.KMeans .\n",
        "\n",
        "How to decide for the value 'K' ?\n",
        "\n",
        "Read the following blog. It provides different ways of evaluating clustering algorithms.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters\n",
        "\n",
        "We will be looking on two methods : Elbow Method, Silhouette Analysis.\n",
        "\n",
        "**Make a list of values for K , ranging from 2 to 10. For each K, fit a model, calculate the inertia and silhouette scores. Plot them. Decide which value of K is optimal !**"
      ],
      "metadata": {
        "id": "GI5spEOLB2Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "Sum_of_squared_distances = []\n",
        "Silhouette_scores = []\n",
        "for num_clusters in range(2,11):\n",
        "  kmeans = KMeans(n_clusters=num_clusters)\n",
        "  kmeans.fit(data)\n",
        "  Sum_of_squared_distances.append(kmeans.inertia_)\n",
        "  score = silhouette_score(data, kmeans.labels_)\n",
        "  Silhouette_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(2,11),Sum_of_squared_distances,marker='o')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Sum of squared distances/Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(2,11),Silhouette_scores,marker='o')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Silhouette_scores')\n",
        "plt.title('Silhouette Analysis For Optimal k')"
      ],
      "metadata": {
        "id": "VvGCAuUW_wzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. KMeans Model Prediction\n",
        "\n",
        "Once you decided the optimal K, once again fit a model with that K value and store the silhouette score and the labels for the entire data.\n",
        "\n",
        "It is observed that the optimal value of k is 4. So, let's store the values of inertia and labels for k=4."
      ],
      "metadata": {
        "id": "1jajN40GH0EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(data)\n",
        "sklearn_kmeans_inertia = kmeans.inertia_\n",
        "sklearn_kmeans_labels = kmeans.labels_\n",
        "sklearn_kmeans_silhoutte_score = silhouette_score(data, kmeans.labels_)\n",
        "\n",
        "print(sklearn_kmeans_inertia,sklearn_kmeans_silhoutte_score)"
      ],
      "metadata": {
        "id": "-ILDOEI8DWsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. KMeans Model Training - Scratch\n",
        "\n",
        "Now, code the KMeans Model from scratch. Train it on the data, and try to find out when you have the labels with maximum accuracy when compared to the labels of the SkLearn model."
      ],
      "metadata": {
        "id": "_fjt3IhXzYgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KMeansScratch:\n",
        "    def __init__(self, k, max_iter=100):\n",
        "        self.k = k\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self.centroids = np.random.randn(self.k, n_features)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "            labels = np.argmin(distances, axis=0)\n",
        "\n",
        "            centroids = np.array([X[labels == k].mean(axis=0) for k in range(self.k)])\n",
        "\n",
        "            if np.allclose(self.centroids, centroids):\n",
        "                break\n",
        "\n",
        "            self.centroids = centroids\n",
        "\n",
        "        self.labels_ = labels\n"
      ],
      "metadata": {
        "id": "sQf3w4l7Ef1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans2 = KMeansScratch(k=3,max_iter=100)\n",
        "kmeans2.fit(np.array(data))\n",
        "scratch_kmeans_labels = kmeans2.labels_\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(sklearn_kmeans_labels,scratch_kmeans_labels))"
      ],
      "metadata": {
        "id": "jUas-61-I3g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. DBSCAN model training - Scikit-Learn\n",
        "\n",
        "Using sklear.clusters.DBSCAN, you have to fit a model on the data.\n",
        "\n",
        "But, here we would like to deal with two hyperparameters : epsilon and minimum number of samples.\n",
        "\n",
        "Make two lists. One with some probable values for epsilon, other with probable values for min_samples.\n",
        "\n",
        "Example : eps= [0.1,0.2,0.5,1,2] , min_samples=[3,4,5,6]\n",
        "\n",
        "Run a nested loop. for each value of eps and min_samples, fit a dbscan model on the data and calculate the silhouette score. Find the parameters for which the silhouette score is maximum."
      ],
      "metadata": {
        "id": "WTWu6y8S0Usv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "eps= [0.1,0.2,0.5,1,2]\n",
        "min_samples=[3,4,5,6]\n",
        "\n",
        "best_params = {'eps':0,'min_samples':0}\n",
        "best_score = 0\n",
        "for min_sample in min_samples:\n",
        "    for ep in eps:\n",
        "        dbscan = DBSCAN(eps=ep,min_samples=min_sample)\n",
        "        dbscan.fit(data)\n",
        "        if (len(np.unique(dbscan.labels_))==1):\n",
        "            continue\n",
        "        score = silhouette_score(data,dbscan.labels_)\n",
        "\n",
        "        if (score>best_score):\n",
        "            best_score=score\n",
        "            best_params['eps']=ep\n",
        "            best_params['min_samples']=min_sample\n",
        "\n",
        "print(best_params)\n",
        "print(best_score)\n"
      ],
      "metadata": {
        "id": "7j7Yi_qgJiGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. DBSCAN model training - Scratch\n",
        "\n",
        "Code the DBScan model. For the same epsilon and min_samples values, fit the model on the data. You should receive the same silhouette score."
      ],
      "metadata": {
        "id": "T77d9N4w1hRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DBSCANScratch:\n",
        "    def __init__(self, eps=0.5, min_samples=5):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Initialize labels array\n",
        "        self.labels_ = np.zeros(X.shape[0])\n",
        "        cluster_label = 0\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            if self.labels_[i] != 0:\n",
        "                continue\n",
        "\n",
        "            neighbors = self.get_neighbors(X, i)\n",
        "            if len(neighbors) < self.min_samples:\n",
        "                self.labels_[i] = -1 # Mark as noise point\n",
        "                continue\n",
        "\n",
        "            cluster_label += 1\n",
        "            self.labels_[i] = cluster_label\n",
        "\n",
        "            # Expand cluster\n",
        "            j = 0\n",
        "            while j < len(neighbors):\n",
        "                neighbor = neighbors[j]\n",
        "\n",
        "                if self.labels_[neighbor] == -1:\n",
        "                    self.labels_[neighbor] = cluster_label\n",
        "\n",
        "                elif self.labels_[neighbor] == 0:\n",
        "                    self.labels_[neighbor] = cluster_label\n",
        "                    new_neighbors = self.get_neighbors(X, neighbor)\n",
        "\n",
        "                    if len(new_neighbors) >= self.min_samples:\n",
        "                        neighbors = np.concatenate((neighbors, new_neighbors))\n",
        "\n",
        "                j += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_neighbors(self, X, i):\n",
        "        distances = np.linalg.norm(X - X[i], axis=1)\n",
        "        return np.where(distances <= self.eps)[0]\n"
      ],
      "metadata": {
        "id": "1oRXZxqAQbWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan2 = DBSCANScratch(eps=0.5,min_samples=3)\n",
        "dbscan2.fit(np.array(data))\n",
        "print(silhouette_score(data,dbscan2.labels_))"
      ],
      "metadata": {
        "id": "84oRUWQyXtfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}