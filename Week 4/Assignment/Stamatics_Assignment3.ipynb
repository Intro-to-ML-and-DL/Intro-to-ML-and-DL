{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : Classification using Naive Bayes\n",
        "\n",
        "Can glucose and blood pressure data classify whether a patient has diabetes or not ? If yes, which classification algorithm should you use ?\n",
        "\n",
        "The dataset **diabetes_classification.csv** has 3 columns and 995 entries with the above data.\n",
        "\n",
        "\n",
        "1. Load the dataset."
      ],
      "metadata": {
        "id": "1aabbIB92YA8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ytm1udmGNwSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The dataset has two feature columns and one target column. Plot a bar graph or histogram showing the distribution of values in the feature columns (count of each value)."
      ],
      "metadata": {
        "id": "vRcqHcd_5XUt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdiNiiJdNy-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The feature column **glucose** has a somewhat Gaussian distribution of data. So we will try out Gaussian Naive Bayes classification for the data using Scikit-Learn.\n",
        "\n",
        "3. Split the dataset.\n",
        "4. Fit a Gaussian NB model on the data. Make predictions and find the accuracy score.\n",
        "\n",
        "Optional :\n",
        "5. Compare the model with other classification algorithms like Logistic Regression, KNN, decision tree etc."
      ],
      "metadata": {
        "id": "Xa-7s9hL5tii"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qc64PVrN1-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 : Regression using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **insurance.csv** file which contain information on insurance charges based on the following informations: age,sex,bmi,region,number of children and whether the person is a smoker or not. You need to predict the charges based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ],
      "metadata": {
        "id": "qSniomr219vK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIzySehxN3nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Separate the numerical and categorical columns.\n",
        "### 3. Label Encode the categorical columns.\n",
        "### 4. Scale the numerical columns. (Scale the charges separately so that you can calculate errors afterwards.)"
      ],
      "metadata": {
        "id": "t3kLtntR2_wm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5BlzbzGN4vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Split the data."
      ],
      "metadata": {
        "id": "KAr0_8zS3d4M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0puLu0rN6Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Support Vector Regressor\n",
        "\n",
        "Here , you will use the SVR model from sklearn.svm and fit it on the training data. Then predict on the test data and calaculate MAE, MSE. But...\n",
        "\n",
        "The SVR class contains many hyperparameters, example : kernel can have the following values : linear, rbf, poly, sigmoid.\n",
        "\n",
        "Use **RandomizedSearchCV** from sklearn.model_selection , create a dictionary with keys 'kernel' and 'gamma' . As values of the keys, create a list of some possible values. Run a 3-fold cross validation test (cv=3) and find the best parameters. Then initiate the SVR model with those parameters."
      ],
      "metadata": {
        "id": "9JKtZ3nN3gmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTXaAbc7N7oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. AdaBoost Regressor\n",
        "\n",
        "We would do similar for AdaBoostRegressor from sklearn.ensemble . Here, the hyperparameters are n_estimators and loss.\n",
        "\n",
        "Instead of RandomizedSearchCV, let's try GridSearchCV . Find the best parameters and then find errors on test data using the model with best parameters."
      ],
      "metadata": {
        "id": "KGt147zA5E6f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5c-EFdtN9Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Now carry the same procedure for Random Forest Regressor and for Gradient Boosting Regression.\n",
        "9. Finally, use <a href=\"https://xgboost.readthedocs.io/en/stable/get_started.html\"> XGBoost Regressor </a> and compare all the models. Comment which model had the least error (MAE and MSE).\n",
        "You will be required to run  <code> !pip install xgboost </code> to import xgboost models."
      ],
      "metadata": {
        "id": "oW11faHs5nH_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bg_amvWHOAJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : Classification using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **bookmyshow_ads.csv** file which contain information on whether an url is spam or not based on 32 features. You need to classify the url as spam or not spam based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ],
      "metadata": {
        "id": "kMMzdz1e3YMp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZTHOA-KOLTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split the data."
      ],
      "metadata": {
        "id": "BcqJmRVyLHFU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b7dtEdwOMnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Comparison\n",
        "\n",
        "Similar to the previous question, use the following classifier models from sklearn and compare them:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Adaboost\n",
        "4. Gradient Boost\n",
        "5. XGBoost\n",
        "\n",
        "For each model, you may also try to find the best hyperparameters using GridSearch Cross Validation or RandomizedSearch Cross Validation."
      ],
      "metadata": {
        "id": "KKX0BSiILMPe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaTC1pJFOPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 : Clustering\n",
        "\n",
        "Customer Segmentation is the subdivision of a market into discrete customer groups that share similar characteristics. Customer Segmentation can be a powerful means to identify unsatisfied customer needs.\n",
        "\n",
        "The csv file **segmentation data.csv** contains basic data about some customers like Customer ID, age, gender, annual income and spending score. You want to classify the customers into different groups so that marketing strategy could be planned in the future accordingly. How many different groups should be made ? What should be the approach ?\n",
        "\n",
        "This is an Unsupervised Learning question since it doesn't provide you with labels - the groups. \n",
        "\n",
        "### 1. Import the necessary modules"
      ],
      "metadata": {
        "id": "AbMIyf8ouk1u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_UKGseStOQgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Read the csv file \"segmentation data.csv\" present in the Github repository as a Pandas DataFrame."
      ],
      "metadata": {
        "id": "FXs9rQC0u3_S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfiso96cORzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Do the necessary preprocessing of the data.\n",
        "\n",
        "> Drop unwanted columns.\n",
        "\n",
        "> Check for null values.\n",
        "\n",
        "> Scale the numerical columns.\n",
        "\n",
        "> Additionally, you may also make the Age column have categorical values. How ? Apply some function that makes age groups turns all ages in some group to a particular number !\n",
        "\n",
        "Note : Don't do everything in a single code block ! Do it step-by-step and show output for each step."
      ],
      "metadata": {
        "id": "pkoXSkGvvEfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8T3S0J0OUS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. KMeans Model Training - Scikit-Learn\n",
        "\n",
        "At first, let's try to implement KMeans Clustering using sklearn.clusters.KMeans .\n",
        "\n",
        "How to decide for the value 'K' ?\n",
        "\n",
        "Read the following blog. It provides different ways of evaluating clustering algorithms.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters\n",
        "\n",
        "We will be looking on two methods : Elbow Method, Silhouette Analysis.\n",
        "\n",
        "**Make a list of values for K , ranging from 2 to 10. For each K, fit a model, calculate the inertia and silhouette scores. Plot them. Decide which value of K is optimal !**"
      ],
      "metadata": {
        "id": "GI5spEOLB2Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onhGIzLqOX8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. KMeans Model Prediction\n",
        "\n",
        "Once you decided the optimal K, once again fit a model with that K value and store the silhouette score and the labels for the entire data.\n",
        "\n",
        "It is observed that the optimal value of k is 4. So, let's store the values of inertia and labels for k=4."
      ],
      "metadata": {
        "id": "1jajN40GH0EP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qydGd1rYOZqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. KMeans Model Training - Scratch\n",
        "\n",
        "Now, code the KMeans Model from scratch. Train it on the data, and try to find out when you have the labels with maximum accuracy when compared to the labels of the SkLearn model."
      ],
      "metadata": {
        "id": "_fjt3IhXzYgz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PT3TD5uqOber"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. DBSCAN model training - Scikit-Learn\n",
        "\n",
        "Using sklear.clusters.DBSCAN, you have to fit a model on the data.\n",
        "\n",
        "But, here we would like to deal with two hyperparameters : epsilon and minimum number of samples.\n",
        "\n",
        "Make two lists. One with some probable values for epsilon, other with probable values for min_samples.\n",
        "\n",
        "Example : eps= [0.1,0.2,0.5,1,2] , min_samples=[3,4,5,6]\n",
        "\n",
        "Run a nested loop. for each value of eps and min_samples, fit a dbscan model on the data and calculate the silhouette score. Find the parameters for which the silhouette score is maximum."
      ],
      "metadata": {
        "id": "WTWu6y8S0Usv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NM5XJSZYOdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. DBSCAN model training - Scratch\n",
        "\n",
        "Code the DBScan model. For the same epsilon and min_samples values, fit the model on the data. You should receive the same silhouette score."
      ],
      "metadata": {
        "id": "T77d9N4w1hRI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ULTd8n1Oc17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}