{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1aabbIB92YA8"
      },
      "source": [
        "# Question 1 : Classification using Naive Bayes\n",
        "\n",
        "Can glucose and blood pressure data classify whether a patient has diabetes or not ? If yes, which classification algorithm should you use ?\n",
        "\n",
        "The dataset **diabetes_classification.csv** has 3 columns and 995 entries with the above data.\n",
        "\n",
        "\n",
        "1. Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytm1udmGNwSl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"diabetes.csv\")\n",
        "\n",
        "# Display the dataset\n",
        "print(dataset.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vRcqHcd_5XUt"
      },
      "source": [
        "2. The dataset has two feature columns and one target column. Plot a bar graph or histogram showing the distribution of values in the feature columns (count of each value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdiNiiJdNy-K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of the feature columns\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot for feature column 1\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(dataset['glucose'])\n",
        "plt.title('Distribution of Glucose')\n",
        "plt.xlabel('Glucose')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Plot for feature column 2\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(dataset['bloodpressure'])\n",
        "plt.title('Distribution of Blood Pressure')\n",
        "plt.xlabel('Blood Pressure')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-7s9hL5tii"
      },
      "source": [
        " The feature column **glucose** has a somewhat Gaussian distribution of data. So we will try out Gaussian Naive Bayes classification for the data using Scikit-Learn.\n",
        "\n",
        "3. Split the dataset.\n",
        "4. Fit a Gaussian NB model on the data. Make predictions and find the accuracy score.\n",
        "\n",
        "Optional :\n",
        "5. Compare the model with other classification algorithms like Logistic Regression, KNN, decision tree etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qc64PVrN1-y"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset[['glucose', 'bloodpressure']]\n",
        "y = dataset['diabetes']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a Gaussian NB model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy score: {:.2f}\".format(accuracy))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qSniomr219vK"
      },
      "source": [
        "# Question 2 : Regression using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **insurance.csv** file which contain information on insurance charges based on the following informations: age,sex,bmi,region,number of children and whether the person is a smoker or not. You need to predict the charges based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzySehxN3nw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('insurance.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t3kLtntR2_wm"
      },
      "source": [
        "### 2. Separate the numerical and categorical columns.\n",
        "### 3. Label Encode the categorical columns.\n",
        "### 4. Scale the numerical columns. (Scale the charges separately so that you can calculate errors afterwards.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5BlzbzGN4vq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "numerical_cols = ['age', 'bmi', 'children']\n",
        "categorical_cols = ['sex', 'region', 'smoker']\n",
        "target_col = 'charges'\n",
        "\n",
        "numerical_data = data[numerical_cols]\n",
        "categorical_data = data[categorical_cols]\n",
        "target_data = data[target_col]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "categorical_data_encoded = categorical_data.copy()\n",
        "for col in categorical_cols:\n",
        "    categorical_data_encoded[col] = label_encoder.fit_transform(categorical_data[col])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaled_numerical_data = scaler.fit_transform(numerical_data)\n",
        "scaled_target_data = scaler.fit_transform(target_data.values.reshape(-1, 1))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KAr0_8zS3d4M"
      },
      "source": [
        "### 5. Split the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0puLu0rN6Gi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the scaled numerical data and encoded categorical data into a single DataFrame\n",
        "X = pd.concat([pd.DataFrame(scaled_numerical_data, columns=numerical_cols), categorical_data_encoded], axis=1)\n",
        "y = scaled_target_data\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKtZ3nN3gmZ"
      },
      "source": [
        "### 6. Support Vector Regressor\n",
        "\n",
        "Here , you will use the SVR model from sklearn.svm and fit it on the training data. Then predict on the test data and calaculate MAE, MSE. But...\n",
        "\n",
        "The SVR class contains many hyperparameters, example : kernel can have the following values : linear, rbf, poly, sigmoid.\n",
        "\n",
        "Use **RandomizedSearchCV** from sklearn.model_selection , create a dictionary with keys 'kernel' and 'gamma' . As values of the keys, create a list of some possible values. Run a 3-fold cross validation test (cv=3) and find the best parameters. Then initiate the SVR model with those parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTXaAbc7N7oS"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'gamma': [0.1, 0.01, 0.001, 0.0001]\n",
        "}\n",
        "\n",
        "# Create an instance of the SVR model\n",
        "svr = SVR()\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(svr, param_distributions=param_grid, cv=3)\n",
        "random_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best parameters found during the search\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Initiate the SVR model with the best parameters\n",
        "svr_best = SVR(kernel=best_params['kernel'], gamma=best_params['gamma'])\n",
        "svr_best.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr_best.predict(X_test)\n",
        "\n",
        "# Calculate MAE and MSE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and evaluation metrics\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KGt147zA5E6f"
      },
      "source": [
        "### 7. AdaBoost Regressor\n",
        "\n",
        "We would do similar for AdaBoostRegressor from sklearn.ensemble . Here, the hyperparameters are n_estimators and loss.\n",
        "\n",
        "Instead of RandomizedSearchCV, let's try GridSearchCV . Find the best parameters and then find errors on test data using the model with best parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5c-EFdtN9Wt"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'loss': ['linear', 'square', 'exponential']\n",
        "}\n",
        "\n",
        "# Create an instance of the AdaBoostRegressor model\n",
        "adaboost = AdaBoostRegressor()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(adaboost, param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best parameters found during the search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Initiate the AdaBoostRegressor model with the best parameters\n",
        "adaboost_best = AdaBoostRegressor(n_estimators=best_params['n_estimators'], loss=best_params['loss'])\n",
        "adaboost_best.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_best.predict(X_test)\n",
        "\n",
        "# Calculate MAE and MSE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and evaluation metrics\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oW11faHs5nH_"
      },
      "source": [
        "8. Now carry the same procedure for Random Forest Regressor and for Gradient Boosting Regression.\n",
        "9. Finally, use <a href=\"https://xgboost.readthedocs.io/en/stable/get_started.html\"> XGBoost Regressor </a> and compare all the models. Comment which model had the least error (MAE and MSE).\n",
        "You will be required to run  <code> !pip install xgboost </code> to import xgboost models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg_amvWHOAJG"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest Regressor\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create an instance of the Random Forest Regressor model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Perform GridSearchCV for Random Forest Regressor\n",
        "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=3)\n",
        "grid_search_rf.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best parameters found during the search\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "\n",
        "# Initiate the Random Forest Regressor model with the best parameters\n",
        "rf_best = RandomForestRegressor(n_estimators=best_params_rf['n_estimators'],\n",
        "                                max_depth=best_params_rf['max_depth'],\n",
        "                                min_samples_split=best_params_rf['min_samples_split'])\n",
        "rf_best.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Make predictions on the test set for Random Forest Regressor\n",
        "y_pred_rf = rf_best.predict(X_test)\n",
        "\n",
        "# Define the hyperparameter grid for Gradient Boosting Regression\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Create an instance of the Gradient Boosting Regression model\n",
        "gb = GradientBoostingRegressor()\n",
        "\n",
        "# Perform GridSearchCV for Gradient Boosting Regression\n",
        "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=3)\n",
        "grid_search_gb.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best parameters found during the search\n",
        "best_params_gb = grid_search_gb.best_params_\n",
        "\n",
        "# Initiate the Gradient Boosting Regression model with the best parameters\n",
        "gb_best = GradientBoostingRegressor(n_estimators=best_params_gb['n_estimators'],\n",
        "                                    learning_rate=best_params_gb['learning_rate'],\n",
        "                                    max_depth=best_params_gb['max_depth'])\n",
        "gb_best.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Make predictions on the test set for Gradient Boosting Regression\n",
        "y_pred_gb = gb_best.predict(X_test)\n",
        "\n",
        "# Create an instance of the XGBoost Regressor model\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# Perform GridSearchCV for XGBoost Regressor\n",
        "grid_search_xgb = GridSearchCV(xgb, param_grid_gb, cv=3)\n",
        "grid_search_xgb.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best parameters found during the search\n",
        "best_params_xgb = grid_search_xgb.best_params_\n",
        "\n",
        "# Initiate the XGBoost Regressor model with the best parameters\n",
        "xgb_best = XGBRegressor(n_estimators=best_params_xgb['n_estimators'],\n",
        "                        learning_rate=best_params_xgb['learning_rate'],\n",
        "                        max_depth=best_params_xgb['max_depth'])\n",
        "xgb_best.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Make predictions on the test set for XGBoost Regressor\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate MAE and MSE for Random Forest Regressor\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate MAE and MSE for Gradient Boosting Regression\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "\n",
        "# Calculate MAE and MSE for XGBoost Regressor\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "\n",
        "# Print the evaluation metrics for all models\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(\"MAE:\", mae_rf)\n",
        "print(\"MSE:\", mse_rf)\n",
        "print()\n",
        "\n",
        "print(\"Gradient Boosting Regression:\")\n",
        "print(\"MAE:\", mae_gb)\n",
        "print(\"MSE:\", mse_gb)\n",
        "print()\n",
        "\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"MAE:\", mae_xgb)\n",
        "print(\"MSE:\", mse_xgb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMzdz1e3YMp"
      },
      "source": [
        "# Question 3 : Classification using SVM and Tree Algorithms\n",
        "\n",
        "In this question, we will be using the **bookmyshow_ads.csv** file which contain information on whether an url is spam or not based on 32 features. You need to classify the url as spam or not spam based on the information given.\n",
        "\n",
        "### 1. Load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZTHOA-KOLTw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"bookmyshow_ads.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BcqJmRVyLHFU"
      },
      "source": [
        "### 2. Split the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b7dtEdwOMnQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KKX0BSiILMPe"
      },
      "source": [
        "### 3. Model Comparison\n",
        "\n",
        "Similar to the previous question, use the following classifier models from sklearn and compare them:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Adaboost\n",
        "4. Gradient Boost\n",
        "5. XGBoost\n",
        "\n",
        "For each model, you may also try to find the best hyperparameters using GridSearch Cross Validation or RandomizedSearch Cross Validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaTC1pJFOPNF"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create the classifier models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"Gradient Boost\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier()\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for GridSearchCV\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\n",
        "        \"max_depth\": [None, 5, 10],\n",
        "        \"min_samples_split\": [2, 5, 10]\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"n_estimators\": [100, 200, 300],\n",
        "        \"max_depth\": [None, 5, 10],\n",
        "        \"min_samples_split\": [2, 5, 10]\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"learning_rate\": [0.1, 1, 10]\n",
        "    },\n",
        "    \"Gradient Boost\": {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"learning_rate\": [0.1, 1, 10],\n",
        "        \"max_depth\": [3, 5, 7]\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"learning_rate\": [0.1, 1, 10],\n",
        "        \"max_depth\": [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV and evaluate models\n",
        "for model_name, model in models.items():\n",
        "    # Perform GridSearchCV with error_score='raise'\n",
        "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, error_score='raise')\n",
        "    try:\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during grid search for {model_name}:\")\n",
        "        print(e)\n",
        "        continue\n",
        "    \n",
        "    # Get the best parameters found during the search\n",
        "    best_params = grid_search.best_params_\n",
        "    \n",
        "    # Initiate the model with the best parameters\n",
        "    model_best = model.set_params(**best_params)\n",
        "    \n",
        "    # Fit the model on the training data\n",
        "    model_best.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions on the test set\n",
        "    y_pred = model_best.predict(X_test)\n",
        "    \n",
        "    # Calculate and print the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(model_name)\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMIyf8ouk1u"
      },
      "source": [
        "# Question 4 : Clustering\n",
        "\n",
        "Customer Segmentation is the subdivision of a market into discrete customer groups that share similar characteristics. Customer Segmentation can be a powerful means to identify unsatisfied customer needs.\n",
        "\n",
        "The csv file **segmentation data.csv** contains basic data about some customers like Customer ID, age, gender, annual income and spending score. You want to classify the customers into different groups so that marketing strategy could be planned in the future accordingly. How many different groups should be made ? What should be the approach ?\n",
        "\n",
        "This is an Unsupervised Learning question since it doesn't provide you with labels - the groups. \n",
        "\n",
        "### 1. Import the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UKGseStOQgk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FXs9rQC0u3_S"
      },
      "source": [
        "### 2. Read the csv file \"segmentation data.csv\" present in the Github repository as a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfiso96cORzB"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"segmentation data.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pkoXSkGvvEfQ"
      },
      "source": [
        "### 3. Do the necessary preprocessing of the data.\n",
        "\n",
        "> Drop unwanted columns.\n",
        "\n",
        "> Check for null values.\n",
        "\n",
        "> Scale the numerical columns.\n",
        "\n",
        "> Additionally, you may also make the Age column have categorical values. How ? Apply some function that makes age groups turns all ages in some group to a particular number !\n",
        "\n",
        "Note : Don't do everything in a single code block ! Do it step-by-step and show output for each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8T3S0J0OUS_"
      },
      "outputs": [],
      "source": [
        "# Drop the 'CustomerID' column as it is not relevant for clustering\n",
        "df = df.drop('ID', axis=1)\n",
        "print(df.head())\n",
        "\n",
        "# Check for null values in the DataFrame\n",
        "print(df.isnull().sum())\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select the numerical columns to be scaled\n",
        "numerical_columns = ['Income', 'Settlement size']\n",
        "\n",
        "# Scale the numerical columns\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "print(df.head())\n",
        "\n",
        "# Define the age groups and corresponding numerical values\n",
        "age_groups = {\n",
        "    '18-30': 1,\n",
        "    '31-40': 2,\n",
        "    '41-50': 3,\n",
        "    '51-60': 4,\n",
        "    '61+': 5\n",
        "}\n",
        "\n",
        "# Function to assign the numerical value based on age group\n",
        "def assign_age_group(age):\n",
        "    if age <= 30:\n",
        "        return age_groups['18-30']\n",
        "    elif age <= 40:\n",
        "        return age_groups['31-40']\n",
        "    elif age <= 50:\n",
        "        return age_groups['41-50']\n",
        "    elif age <= 60:\n",
        "        return age_groups['51-60']\n",
        "    else:\n",
        "        return age_groups['61+']\n",
        "\n",
        "# Apply the age group function to the 'Age' column\n",
        "df['Age'] = df['Age'].apply(assign_age_group)\n",
        "print(df.head())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GI5spEOLB2Sw"
      },
      "source": [
        "### 4. KMeans Model Training - Scikit-Learn\n",
        "\n",
        "At first, let's try to implement KMeans Clustering using sklearn.clusters.KMeans .\n",
        "\n",
        "How to decide for the value 'K' ?\n",
        "\n",
        "Read the following blog. It provides different ways of evaluating clustering algorithms.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters\n",
        "\n",
        "We will be looking on two methods : Elbow Method, Silhouette Analysis.\n",
        "\n",
        "**Make a list of values for K , ranging from 2 to 10. For each K, fit a model, calculate the inertia and silhouette scores. Plot them. Decide which value of K is optimal !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onhGIzLqOX8p"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the range of values for K\n",
        "k_values = range(2, 11)\n",
        "\n",
        "# Initialize lists to store inertia and silhouette scores for each K\n",
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "# Iterate over each value of K\n",
        "for k in k_values:\n",
        "    # Initialize and fit the K-means model\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(df)\n",
        "    \n",
        "    # Calculate the inertia and silhouette scores\n",
        "    inertia = kmeans.inertia_\n",
        "    silhouette = silhouette_score(df, kmeans.labels_)\n",
        "    \n",
        "    # Append the scores to the respective lists\n",
        "    inertia_scores.append(inertia)\n",
        "    silhouette_scores.append(silhouette)\n",
        "\n",
        "# Plot the inertia scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, inertia_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method - Inertia vs. Number of Clusters')\n",
        "plt.show()\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis - Score vs. Number of Clusters')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1jajN40GH0EP"
      },
      "source": [
        "### 5. KMeans Model Prediction\n",
        "\n",
        "Once you decided the optimal K, once again fit a model with that K value and store the silhouette score and the labels for the entire data.\n",
        "\n",
        "It is observed that the optimal value of k is 4. So, let's store the values of inertia and labels for k=4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qydGd1rYOZqx"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Fit the K-means model with K=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(df)\n",
        "\n",
        "# Calculate the silhouette score for the entire data\n",
        "silhouette = silhouette_score(df, kmeans.labels_)\n",
        "\n",
        "# Store the silhouette score and labels\n",
        "silhouette_score_4 = silhouette\n",
        "labels_4 = kmeans.labels_\n",
        "\n",
        "# Print the silhouette score for K=4\n",
        "print(\"Silhouette Score for K=4:\", silhouette_score_4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_fjt3IhXzYgz"
      },
      "source": [
        "### 6. KMeans Model Training - Scratch\n",
        "\n",
        "Now, code the KMeans Model from scratch. Train it on the data, and try to find out when you have the labels with maximum accuracy when compared to the labels of the SkLearn model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT3TD5uqOber"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class KMeansScratch:\n",
        "    def __init__(self, n_clusters, max_iter=100):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "    \n",
        "    def fit(self, X):\n",
        "        self.centroids = X[np.random.choice(X.shape[0], size=self.n_clusters, replace=False)]\n",
        "        \n",
        "        for _ in range(self.max_iter):\n",
        "            # Assign points to the nearest centroid\n",
        "            labels = self._assign_labels(X)\n",
        "            \n",
        "            # Update centroids based on assigned points\n",
        "            self._update_centroids(X, labels)\n",
        "        \n",
        "        self.labels = labels\n",
        "    \n",
        "    def _assign_labels(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "        return labels\n",
        "    \n",
        "    def _update_centroids(self, X, labels):\n",
        "        for i in range(self.n_clusters):\n",
        "            self.centroids[i] = np.mean(X[labels == i], axis=0)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "        return labels\n",
        "\n",
        "\n",
        "# Initialize and fit the K-means model from scratch\n",
        "kmeans_scratch = KMeansScratch(n_clusters=4)\n",
        "kmeans_scratch.fit(df.values)\n",
        "\n",
        "# Calculate the accuracy of the scratch model compared to the sklearn model\n",
        "accuracy = np.mean(kmeans_scratch.labels == labels_4)\n",
        "print(\"Accuracy of K-means Scratch Model:\", accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WTWu6y8S0Usv"
      },
      "source": [
        "### 7. DBSCAN model training - Scikit-Learn\n",
        "\n",
        "Using sklear.clusters.DBSCAN, you have to fit a model on the data.\n",
        "\n",
        "But, here we would like to deal with two hyperparameters : epsilon and minimum number of samples.\n",
        "\n",
        "Make two lists. One with some probable values for epsilon, other with probable values for min_samples.\n",
        "\n",
        "Example : eps= [0.1,0.2,0.5,1,2] , min_samples=[3,4,5,6]\n",
        "\n",
        "Run a nested loop. for each value of eps and min_samples, fit a dbscan model on the data and calculate the silhouette score. Find the parameters for which the silhouette score is maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM5XJSZYOdmB"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Define the lists of probable values for epsilon and min_samples\n",
        "eps_values = [0.1, 0.2, 0.5, 1, 2]\n",
        "min_samples_values = [2, 3, 4, 5, 6]\n",
        "\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "max_silhouette_score = -1\n",
        "\n",
        "# Iterate over each value of eps and min_samples\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # Initialize and fit the DBSCAN model\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        dbscan.fit(df)\n",
        "        \n",
        "        # Check if more than one label is generated\n",
        "        unique_labels = len(set(dbscan.labels_))\n",
        "        if unique_labels > 1:\n",
        "            # Calculate the silhouette score\n",
        "            silhouette = silhouette_score(df, dbscan.labels_)\n",
        "            \n",
        "            # Update the best parameters if the silhouette score is higher\n",
        "            if silhouette > max_silhouette_score:\n",
        "                max_silhouette_score = silhouette\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "\n",
        "# Print the best parameters and the corresponding silhouette score\n",
        "print(\"Best Parameters: eps =\", best_eps, \", min_samples =\", best_min_samples)\n",
        "print(\"Max Silhouette Score:\", max_silhouette_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T77d9N4w1hRI"
      },
      "source": [
        "### 8. DBSCAN model training - Scratch\n",
        "\n",
        "Code the DBScan model. For the same epsilon and min_samples values, fit the model on the data. You should receive the same silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ULTd8n1Oc17"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DBSCANScratch:\n",
        "    def __init__(self, eps, min_samples):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "    \n",
        "    def fit(self, X):\n",
        "        self.labels = np.zeros(len(X), dtype=int)\n",
        "        self.cluster_id = 0\n",
        "        \n",
        "        for i in range(len(X)):\n",
        "            if self.labels[i] == 0:\n",
        "                if self._expand_cluster(X, i):\n",
        "                    self.cluster_id += 1\n",
        "    \n",
        "    def _expand_cluster(self, X, i):\n",
        "        neighbors = self._region_query(X, i)\n",
        "        \n",
        "        if len(neighbors) < self.min_samples:\n",
        "            self.labels[i] = -1\n",
        "            return False\n",
        "        \n",
        "        self.labels[i] = self.cluster_id\n",
        "        \n",
        "        for neighbor in neighbors:\n",
        "            if self.labels[neighbor] == 0:\n",
        "                self.labels[neighbor] = self.cluster_id\n",
        "                \n",
        "                neighbor_neighbors = self._region_query(X, neighbor)\n",
        "                if len(neighbor_neighbors) >= self.min_samples:\n",
        "                    neighbors = np.append(neighbors, neighbor_neighbors)\n",
        "            \n",
        "            if self.labels[neighbor] == -1:\n",
        "                self.labels[neighbor] = self.cluster_id\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def _region_query(self, X, i):\n",
        "        return np.where(np.linalg.norm(X - X[i], axis=1) <= self.eps)[0]\n",
        "\n",
        "# Fit the DBSCAN model from scratch with the same epsilon and min_samples values\n",
        "dbscan_scratch = DBSCANScratch(eps=2, min_samples=4)\n",
        "dbscan_scratch.fit(df.values)\n",
        "\n",
        "# Calculate the silhouette score for the scratch model\n",
        "silhouette_scratch = silhouette_score(df, dbscan_scratch.labels)\n",
        "print(\"Silhouette Score (Scratch Model):\", silhouette_scratch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
